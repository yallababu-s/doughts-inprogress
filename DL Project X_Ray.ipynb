{
 "cells": [
  {
   "cell_type": "raw",
   "id": "497130db-844a-461d-9d78-f223d550a0e0",
   "metadata": {},
   "source": [
    "PROBLEM STATEMENT:\n",
    "\n",
    "This dataset contains X-ray images of patients' chests along with labels indicating whether they have pneumonia or not.\n",
    "\n",
    "\n",
    "Data: https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia\n",
    "\n",
    "Data Understanding and Preprocessing:\n",
    "First, you'll need to understand the structure of the dataset, including the format of the images and the corresponding labels.\n",
    "Preprocess the images (resize, normalization, etc.) and split the data into training, validation, and testing sets.\n",
    "Artificial Neural Networks (ANN):\n",
    "Start with a basic ANN architecture to classify the X-ray images into two classes: pneumonia or non-pneumonia.\n",
    "Train the ANN using the preprocessed images and evaluate its performance.\n",
    "Convolutional Neural Networks (CNN):\n",
    "CNNs are particularly well-suited for image classification tasks. Use a CNN architecture to improve the classification accuracy.\n",
    "Experiment with different CNN architectures and fine-tune hyperparameters to achieve better results.\n",
    "\n",
    "\n",
    "Long Short-Term Memory (LSTM):\n",
    "LSTMs are a type of RNN that are well-suited for sequential data with long-range dependencies. You could use LSTMs to analyze temporal patterns in patient data, such as changes in X-ray images over time or correlations with other clinical variables.\n",
    "Autoencoders:\n",
    "Autoencoders can be used for feature extraction or dimensionality reduction. You could pre-train an autoencoder on the X-ray images to learn a compact representation of the data, which can then be used as input to other models like CNNs or LSTMs.\n",
    "For the case study, you can analyze the performance of each model in terms of accuracy, precision, recall, and F1-score. Additionally, you can explore techniques for model interpretability, such as class activation maps or attention mechanisms, to understand which parts of the X-ray images are most important for classification.\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b2265fee-2405-43ac-b7c9-bc5db714dc59",
   "metadata": {},
   "source": [
    "Solution:\n",
    "\n",
    "classifying pneumonia using chest X-ray images! Here's how you can structure your workflow in detail:\n",
    "\n",
    "Step 1: Data Understanding and Preprocessing\n",
    "Data Exploration:\n",
    "\n",
    "Load the dataset and explore its structure.\n",
    "Check for any class imbalance (number of pneumonia vs. non-pneumonia samples).\n",
    "Visualize a few X-ray images to get a sense of the data.\n",
    "Preprocessing:\n",
    "\n",
    "Resize Images: Resize all images to a consistent dimension (e.g., 128x128 or 224x224) for input into neural networks.\n",
    "Normalization: Normalize pixel values to a range of [0, 1] or [-1, 1].\n",
    "Label Encoding: Convert labels into numerical format (e.g., 0 for non-pneumonia, 1 for pneumonia).\n",
    "Data Splitting: Split the dataset into training, validation, and testing sets (e.g., 70% training, 15% validation, 15% testing).\n",
    "Step 2: Artificial Neural Networks (ANN)\n",
    "Architecture Design:\n",
    "\n",
    "Flatten the image data into a 1D vector.\n",
    "Create a feedforward ANN with input, hidden, and output layers.\n",
    "Use activation functions like ReLU for hidden layers and sigmoid/softmax for the output layer.\n",
    "Training and Evaluation:\n",
    "\n",
    "Compile the model with a binary cross-entropy loss function and an optimizer like Adam.\n",
    "Train the model and monitor performance using validation accuracy and loss.\n",
    "Evaluate the model on the test set.\n",
    "Challenges:\n",
    "\n",
    "ANN may not perform well due to the lack of spatial feature extraction.\n",
    "Step 3: Convolutional Neural Networks (CNN)\n",
    "Basic CNN Architecture:\n",
    "\n",
    "Use convolutional layers with filters, followed by pooling layers (e.g., MaxPooling).\n",
    "Add fully connected layers at the end.\n",
    "Use dropout layers to prevent overfitting.\n",
    "Advanced Architectures:\n",
    "\n",
    "Experiment with deeper networks like ResNet, VGG, or MobileNet.\n",
    "Consider transfer learning by using pre-trained models on ImageNet.\n",
    "Hyperparameter Tuning:\n",
    "\n",
    "Adjust learning rates, number of filters, kernel sizes, and batch sizes.\n",
    "Use techniques like learning rate schedulers or early stopping.\n",
    "Evaluation Metrics:\n",
    "\n",
    "Use accuracy, precision, recall, F1-score, and a confusion matrix.\n",
    "Visualize class activation maps (CAM) to interpret which regions of the X-ray influenced predictions.\n",
    "Step 4: Long Short-Term Memory (LSTM)\n",
    "Application Context:\n",
    "\n",
    "If there are sequential X-ray images for each patient over time, preprocess the data into sequences.\n",
    "Feed the sequences into an LSTM network to analyze temporal patterns.\n",
    "LSTM Model Design:\n",
    "\n",
    "Use LSTM layers followed by dense layers for classification.\n",
    "Ensure time-series data is properly formatted as input.\n",
    "Step 5: Autoencoders\n",
    "Feature Extraction:\n",
    "\n",
    "Train an autoencoder to learn compact representations of X-ray images.\n",
    "Use the encoder part to extract features, then feed these into a CNN or another classifier.\n",
    "Anomaly Detection:\n",
    "\n",
    "Use autoencoders for unsupervised anomaly detection (e.g., detect pneumonia as anomalies in non-pneumonia data).\n",
    "Step 6: Model Evaluation and Comparison\n",
    "Metrics:\n",
    "\n",
    "Compare models based on accuracy, precision, recall, and F1-score.\n",
    "Use Receiver Operating Characteristic (ROC) curves and Area Under the Curve (AUC) for additional insights.\n",
    "Interpretability:\n",
    "\n",
    "Use techniques like Grad-CAM or saliency maps to understand model focus.\n",
    "Explore attention mechanisms if applicable.\n",
    "Step 7: Deployment\n",
    "Once a model performs well, deploy it using frameworks like Flask, FastAPI, or TensorFlow Serving.\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0525c801-6311-48e9-a3b9-50e5d5f2672f",
   "metadata": {},
   "source": [
    "#vedeep is the virtual environment installed libraries\n",
    "conda install Python == 3.11.0\n",
    "conda install Numpy==1.24.3\n",
    "conda installPandas==2.2.1\n",
    "conda installScikit-learn==1.4.2\n",
    "conda install Matplotlib==3.7.2\n",
    "conda install Pydot==2.0.0\n",
    "conda install Tensorflow==2.12.1\n",
    "conda install Seaborn==0.13.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5cb21c8-bbe0-4f8a-9fc0-32886af8a7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, LSTM, TimeDistributed, GlobalAveragePooling2D, Input\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3e69628-a54e-437b-b952-7ed7edd74b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#un zip the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "38a5d351-f42b-4e58-8e0d-8baeb79e093d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "# Path to the ZIP file\n",
    "zip_file_path = 'X_ray.zip'\n",
    "# Folder to extract files to\n",
    "extracted_folder_path = 'extracted_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4f1d1b40-7a23-4fe8-ae68-785a8ec418ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract ZIP file\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extracted_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6700b656-76e2-4d35-a620-4b930d12d7a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['chest_xray']\n"
     ]
    }
   ],
   "source": [
    "# List the extracted files\n",
    "extracted_files = os.listdir(extracted_folder_path)\n",
    "print(extracted_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1772ba9-64b4-4d3e-97f2-d101d1139518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted files: ['chest_xray']\n"
     ]
    }
   ],
   "source": [
    "# List the extracted files\n",
    "print(f\"Extracted files: {os.listdir(extracted_folder_path)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7775998-8931-4b8d-98bc-edbcfb575265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "IMG_SIZE = (128, 128)  # Resize images to 128x128\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf67afa0-7687-43e5-acdb-e963317e9d9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14055 images belonging to 1 classes.\n",
      "Found 3513 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the data\n",
    "train_datagen = ImageDataGenerator(rescale=1.0/255, validation_split=0.2)  # Normalize pixel values\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    extracted_folder_path,  # Use the correct directory path\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    extracted_folder_path,  # Use the correct directory path\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    subset='validation'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dfac7bd5-f50f-4c08-9e52-d6c7228c429b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14055 images belonging to 1 classes.\n",
      "Found 3513 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the data\n",
    "train_datagen = ImageDataGenerator(rescale=1.0/255, validation_split=0.2)  # Normalize pixel values\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    extracted_folder_path,  # Use the correct directory path\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    extracted_folder_path,  # Use the correct directory path\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    subset='validation'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8a426d0-4146-467b-b566-efc7038aca6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "24cc1029-6664-40b0-9b6c-1ba68ffd6527",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Build the ANN model (CNN example for image data)\n",
    "ann_model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')  # For binary classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "#model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ff8f0a7e-e976-4423-bf0f-959639c06685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ ANN Model ------------------\n",
    "# Flatten images for ANN input\n",
    "#ann_model = Sequential([\n",
    "    #Flatten(input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3)),\n",
    "   # Dense(128, activation='relu'),\n",
    "   # Dropout(0.5),\n",
    "   # Dense(1, activation='sigmoid')\n",
    "#])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "45b76fa6-51a1-49a9-833e-9ee760f2bfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile ANN\n",
    "#ann_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "#Compiling the model\n",
    "ann_model.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer=\"adam\",\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "# Train ANN\n",
    "#history=model.fit(X_train_digit, Y_train_digit, batch_size=100, epochs=10,validation_data=(X_test_digit, Y_test_digit)) for minst dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2fd74c0b-6481-42d2-9a1f-3ae18cabe5d8",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnidentifiedImageError",
     "evalue": "cannot identify image file <_io.BytesIO object at 0x000002DB6D972360>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnidentifiedImageError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 12\u001b[0m\n\u001b[0;32m      8\u001b[0m validation_steps \u001b[38;5;241m=\u001b[39m validation_samples \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m batch_size\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mann_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Number of batches per epoch\u001b[39;49;00m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Number of validation batches\u001b[39;49;00m\n\u001b[0;32m     18\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\vedeep\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\vedeep\\lib\\site-packages\\PIL\\Image.py:3536\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3534\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message)\n\u001b[0;32m   3535\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot identify image file \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (filename \u001b[38;5;28;01mif\u001b[39;00m filename \u001b[38;5;28;01melse\u001b[39;00m fp)\n\u001b[1;32m-> 3536\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m UnidentifiedImageError(msg)\n",
      "\u001b[1;31mUnidentifiedImageError\u001b[0m: cannot identify image file <_io.BytesIO object at 0x000002DB6D972360>"
     ]
    }
   ],
   "source": [
    "# Assume you know the total number of samples\n",
    "train_samples = 1000  # Replace this with the number of samples in your training set\n",
    "validation_samples = 200  # Replace this with the number of samples in your validation set\n",
    "batch_size = 32\n",
    "\n",
    "# Calculate steps per epoch and validation steps\n",
    "steps_per_epoch = train_samples // batch_size\n",
    "validation_steps = validation_samples // batch_size\n",
    "\n",
    "\n",
    "# Train the model\n",
    "history = ann_model.fit(\n",
    "    train_generator,\n",
    "    epochs=EPOCHS,\n",
    "    steps_per_epoch=steps_per_epoch,  # Number of batches per epoch\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_steps  # Number of validation batches\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f462f83c-2d46-4f84-9544-1ab44436bbab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238b306b-ba97-42b3-8e14-aeb851029937",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "446ac8c6-13a5-4813-b15a-4eb99af2b38a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'generator' object has no attribute 'samples'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Train model with the generator\u001b[39;00m\n\u001b[0;32m      2\u001b[0m ann_history \u001b[38;5;241m=\u001b[39m ann_model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m      3\u001b[0m     train_generator,\n\u001b[0;32m      4\u001b[0m     epochs\u001b[38;5;241m=\u001b[39mEPOCHS,\n\u001b[1;32m----> 5\u001b[0m     steps_per_epoch\u001b[38;5;241m=\u001b[39m\u001b[43mtrain_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msamples\u001b[49m \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m BATCH_SIZE,\n\u001b[0;32m      6\u001b[0m     validation_data\u001b[38;5;241m=\u001b[39mvalidation_generator,\n\u001b[0;32m      7\u001b[0m     validation_steps\u001b[38;5;241m=\u001b[39mvalidation_generator\u001b[38;5;241m.\u001b[39msamples \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m BATCH_SIZE\n\u001b[0;32m      8\u001b[0m )\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'generator' object has no attribute 'samples'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Train model with the generator\n",
    "ann_history = ann_model.fit(\n",
    "    train_generator,\n",
    "    epochs=EPOCHS,\n",
    "    steps_per_epoch=train_generator.samples // BATCH_SIZE,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969c3f2a-c84b-459b-ac61-04bf1f294ec2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8efa88f-b104-475c-99f6-444c2fc72c27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfe3d1f-6a92-4c86-bf00-0711b0dc357a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21204b4-5316-49aa-8d93-fb1dc416cf55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5aca30-825f-43b0-9915-0fb7f04d4222",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c88da45-0995-4e91-b97f-2d3bd19fe691",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4352c5-05ee-496a-a565-bd7f784e5529",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd075aef-def1-41c7-931a-4034852ca297",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd07b3d-24a8-4083-8a05-81c98cd6fab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install opencv-python"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
